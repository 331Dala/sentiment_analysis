{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c945797-bb36-4d5e-acc5-ac0855272062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (2.2.4)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from tweepy) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.56.0-cp310-cp310-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.1.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dala\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Using cached matplotlib-3.10.1-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.56.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Using cached pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pandas-2.2.3 pillow-11.1.0 pyparsing-3.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fdb04944-c3db-4e73-b8aa-89dfea1aec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6734820-a55a-4e37-b8b2-588bd1aee0ba",
   "metadata": {},
   "source": [
    "1.Authenticate to Twitter(Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4fdfee04-7df1-48f3-b32e-fa3eda7fd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer_key = 'uS37Y3WSZC2KRU4PHT8zqSqJd'\n",
    "# consumer_secret = 'rTeP2hPWQdtRaDLV76kcmjR8RD0rYL6NwWHn7ABFIU4dEgYgKZ'\n",
    "# access_token = '1875905360898469888-pdiCYt9cU2X9LSOTp93vZodS488YeV'\n",
    "# access_token_secret = '9jIOsf8U9wHzFoWwgQ572i6FDPG3JEeEsSo4ahjIckcis'\n",
    "# # does not have permission to access the endpoint so try bearer_token\n",
    "# bearer_token = 'AAAAAAAAAAAAAAAAAAAAAFBP0QEAAAAACf68r3wbw%2BVsQXZqcMg81jwvD4Y%3DsJr72ZUKK56UMOMJgLOPHqbmNI6N7ysy1HgcVY6SkGXu7EbDQZ'\n",
    "# client = tw.Client(bearer_token=bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f3570b74-7310-4f2b-9189-6cb13aa803c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenticate\n",
    "# auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a167a4-b3ec-4657-b03d-6c216684c5bd",
   "metadata": {},
   "source": [
    "2.Get Tweets(From local dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "312672a6-4d84-4718-9534-9b113f49ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_tag = '#presidentialdebate'\n",
    "# query = tw.Cursor(api.search_tweets, q=hash_tag).items(10)\n",
    "# tweets = [{'Tweets': tweet.text, 'TimeStamp': tweet.created_at} for tweet in query]\n",
    "# print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7b4dedc5-bde0-47aa-a8f4-1f334fb05b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"#presidentialdebate\"\n",
    "\n",
    "# # 搜索最近的推文（注意：max_results 单页最大为 100 if we query more will get TooManyRequests: 429 Too Many Requests Too Many Requests）\n",
    "# response = client.search_recent_tweets(query=query, max_results=100)\n",
    "\n",
    "# if response.data:\n",
    "#     tweets = [{'Tweets': tweet.text, 'TimeStamp': tweet.created_at} for tweet in response.data]\n",
    "#     for tweet in tweets:\n",
    "#         print(tweet)\n",
    "# else:\n",
    "#     print(\"No relevant tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6d2fb1a7-9113-4edb-9680-37b9050f659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['created_at', 'tweet']\n",
    "cutoff_date = '2020-10-31 23:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "17fc1ba6-39f4-4577-8ae0-0e7ae3a4317d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhashtag_donaldtrump.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('hashtag_donaldtrump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cfdf0546-3f6f-40e1-96c9-e4a2462fbb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dala\\AppData\\Local\\Temp\\ipykernel_16128\\2951312761.py:1: DtypeWarning: Columns (1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('hashtag_joebiden.csv')\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('hashtag_joebiden.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "39f9544e-fae4-424d-a238-28e6e9fd29ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhashtag_donaldtrump.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashtag_joebiden.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df1[columns_to_keep]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sentiment_analysis\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n"
     ]
    }
   ],
   "source": [
    "df1 = df1[columns_to_keep]\n",
    "df2 = df2[columns_to_keep]\n",
    "# 将日期列转换为日期时间类型\n",
    "df1['created_at'] = pd.to_datetime(df1['created_at'])\n",
    "df2['created_at'] = pd.to_datetime(df2['created_at'])\n",
    "\n",
    "# 删除截止日期之后的数据\n",
    "df1 = df1[df1['created_at'] <= pd.to_datetime(cutoff_date)]\n",
    "df2 = df2[df2['created_at'] <= pd.to_datetime(cutoff_date)]\n",
    "# 合并两个 DataFrame\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 保存合并后的 DataFrame 到新的 CSV 文件\n",
    "output_file = 'combined_file.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"合并后的数据已保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745cbd9-bb08-4b91-98e3-dc1748274d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e22f162-6ed7-4fa5-ad8f-2326a7a5b05d",
   "metadata": {},
   "source": [
    "3.Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec19c88-0d47-45bf-8703-655d6b44e0b4",
   "metadata": {},
   "source": [
    "4.Caculate Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e447e9-ce62-416b-bfd6-aafcd0c30e6b",
   "metadata": {},
   "source": [
    "5.Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5aa857-ba11-432e-aec6-0969c33f5763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentiment_analysis]",
   "language": "python",
   "name": "conda-env-sentiment_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
